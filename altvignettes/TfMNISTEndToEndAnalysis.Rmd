---
title: "An example of Stacked Autoencoder"
author: "F.J. Valverde-Albacete and C. Pelaez-Moreno"
date: "11/28/2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This vignette tries to reproduce in Keras the Stacked Autoencoder of 
([Yu and Principe, 2018](https://arxiv.org/abs/1804.00057))

## Installation

```{r general-libraries, echo=FALSE}
library(tidyverse)
library(entropies)
#library(tensorflow)#cascaded by keras
#install_tensorflow()
```

```{r keras-installation, echo=FALSE}
library(keras)
use_session_with_seed(27)# for reproducibility
#from https://cran.r-project.org/web/packages/keras/vignettes/faq.html
#Please note again that use_session_with_seed() disables GPU computations and CPU parallelization by default (as both can lead to non-deterministic computations) so should generally not be used when model training time is paramount. You can re-enable GPU computations and/or CPU parallelism using the disable_gpu and disable_parallel_cpu arguments. For example:
#use_session_with_seed(42, disable_gpu = FALSE, disable_parallel_cpu = FALSE)

#install_keras()#Also installs tensorflow.
#TODO: change to install it locally through Anaconda
GPU <- TRUE
GPU <- FALSE
#check this to enable GPU
use_session_with_seed(27, 
                      disable_gpu = !GPU,
                      disable_parallel_cpu = GPU
                      )
```


# Data loading and preparation

```{r load-data}
mnist <- dataset_mnist()#Accesses aws!

x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 28*28))
x_test <- array_reshape(x_test, c(nrow(x_test), 28*28))
```

Try to transform the observation data to continuous form.

```{r data-massaging}
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

To analyze this data, see the vignette TfMNISTSourceAnalysis.Rmd

# Exploring a Stacked Autoencoder with Entropies

## Defining the model

From [Yu and Principe, 2018]:

`The topology of SAEs on MNIST and
Fashion-MNIST is fixed to be “784-1000-500-250-K-250-500-1000-784”
* The first layer is the actual 28 x 28 image linearized.
* The K represents a variable number of neurons designed to enforce the information bottleneck with K = 2, 11, 15 and 32 

(N.b.: Although it might be 36, following the illustration 6 in the paper).

Define the actual model by creating layers for all of those dimensions: 

```{r experiment controls}
# The dimensions of the intermediate layers
dimInput <- ncol(x_train)
dims <- c(1000,500,250) #and backwards
# The dimensions of the bottlenecks
bottleneckDims <- c(4)#ceil log(num_classes) 
#bottleneckDims <- c(2,4) # tiny test: c(4)#ceil log(num_classes) 
#bottleneckDims <- c(2,4,11,15,32)#Yu & Principe's + 4
# bottleneckDims <- 1:10 # Quick exploration of first orders
#
# The training control, e.g. see keras::fit
epochs <- 1
#epochs <- 2
# epochs <- 5
# epochs <- 10
batch_size <- 100
shuffle <- TRUE
# Preparing the entropic evaluation
dsX <- infotheo::discretize(x_test, disc="equalwidth")
```

Create the names of the layers in order: these will create a kind of forward references pointing at what to observe in the model. 

```{r}
# The names of the layers come from Yu & Principe, 2018
layerNames <- c("X",paste0("T",1:(length(dims))))
layerNames <- c(
    layerNames, "Z", sprintf("%sp", rev(layerNames))
    )
sprintf("Layer names: %s", paste(layerNames, collapse=", "))
```

## The Stacked Autoencoder itself 

A general keras architecture allows for more symmetry in the model:

```{r SAE-functional, echo=FALSE}
numLayers <- length(layerNames)
# Make room for som training and testing results
numBottleneckDims <- length(bottleneckDims)
historyList <- vector('list', numBottleneckDims)#keeps the training
testList <- vector('list', numBottleneckDims)#keeps the testing metrics
saeEntropies <- tibble()
historyDf <- tibble() # The aggregate of all the experiments
#First create the layers in a list then build the model with the list
for(i in 1:numBottleneckDims){# index into bottleneck dimensions
    k <- bottleneckDims[i]
    # Dimensions coindexed with layerNames
    layerDims <- c(dimInput, dims)
    layerDims <- c(layerDims, k, rev(layerDims))
    logName <- sprintf("ASE_%s_log", paste(layerDims, collapse="_"))
    # create and populate list of layers for this BNdim
    layerList <- vector('list', numLayers)
    layerList[[1]] <- #start the model with an input layer
        layer_input(
            name = layerNames[[1]],
            shape = dimInput
        )
    #to go over intermediate layers
    for(j in 2:numLayers){
        layerList[[j]] <- #create and store layer
            layer_dense( object=layerList[[j-1]],
                name = layerNames[[j]], 
                units = layerDims[j], 
                #activation = 'relu'
                activation="sigmoid"
                )
     }

    # Models to be analysed
    # Build the model from input to output
    SAE <- keras_model(layerList[[1]], layerList[[numLayers]])
    summary(SAE)
    # Build the model from input to middle layer
    SAEtransform <- keras_model(layerList[[1]], 
                                layerList[[ceiling(numLayers/2)]])
    #summary(SAEtransform)
    
    # Compile the model, following Yu & Principe's recipe
    SAE %>% compile(
        #optimizer='adadelta',#sgd
        optimize="sgd", #stochastic gradient descent
        #loss='binary_crossentropy'
        loss='mse',  # mean square error
        metrics=list("mae") # extra: mean absolute error
    )

    # Training
    historyList[[i]] <- 
        SAE %>% fit(
            x=x_train, 
            y=x_train, # same target as input, since this is an AE
            epochs=epochs, #similar to Principe et al. settings
            batch_size = batch_size,
            shuffle,
            # use stream loss, etc, epoch results to a csv file
            # callbacks = list(
            #     callback_csv_logger(
            #         #"MNIST-sae-Z_%d-e-{epoch:02d}.csv",
            #         sprintf("MNIST-sae-Z_%d-results.csv",
            #                 bottleneckDims[i]),
            #         append=TRUE)
            # ),
            # # write the model on each epoch, for further description
            # callbacks=list(callback_model_checkpoint(
            #     sprintf("MNIST-sae-Z_%d-e-{epoch:02d}.hdf5",
            #             bottleneckDims[i]),
            #     verbose=1
            #     )
            # ),
            validation_split = 1/6
            #validation_data = list(x_test, x_test)
        )
    historyDf <- rbind(
        historyDf, 
        cbind(
            nb = bottleneckDims[i],
            as.data.frame(historyList[[i]])
        )
    )
    #The history object returned by fit() includes loss and accuracy 
    #metrics which we can plot:
    plot(historyList[[i]])
    
    #Evaluate the model performance on the training metric:
    testList[[i]] <- SAE %>% evaluate(x_test, x_test)
    print(testList[[i]])

    # Generate predictions on test data
    x_test_prime <- SAE %>% predict(x_test)
    z_test <- SAEtransform %>% predict(x_test)
    
    # Working out the entropies
    dsXprime <- infotheo::discretize(x_test_prime, disc="equalwidth")
    saeEntropies <- rbind(
        saeEntropies,
        cbind(
            bnDim = k,
            endpoint = layerNames[numLayers],
           jentropies(dsX,dsXprime)#pretty quick!,
        )
    )
    dsZ <- infotheo::discretize(z_test, disc="equalwidth")
    saeEntropies <- rbind(
        saeEntropies,
        cbind(
            bnDim = k,
            endpoint = "Z",
           jentropies(dsX,dsZ)#pretty quick!,
        )
    )
}
```

Now we visualize the efficiency of changing the dimension of the bottleneck in terms of the entropy transmitted to the final stage.

```{r assess-info-xmission}
fancy <- TRUE
#fancy <- FALSE
totalEntropies <- saeEntropies %>% filter(endpoint== "Xp", type=="Y")
cmetXprime <- ggmetern(totalEntropies,fancy) +
    geom_point(aes(color=bnDim, shape=type), size=3)
cmetXprime
```

```{r assessmnt-info-encoding}
encodingEntropies <- saeEntropies %>% 
    filter(endpoint== "Z", type=="Y")
cmetZ <- ggmetern(encodingEntropies,fancy) +
    geom_point(aes(color=bnDim, shape=type), size=3)
cmetZ <- cmetZ + transition_states(encodingEntropies$bnDim,1,1,TRUE)
animate(cmetZ)
```
Intentar una animación de las 

```{r}


```

The normalized entropies seem to be worth plotting


```{r M_P_vs_H_U}
encodingEntropies <- encodingEntropies %>% 
    mutate(normM_P = encodingEntropies$M_P/encodingEntropies$H_U)
ggplot(encodingEntropies, aes(x=bottleneckDims, y=H_U)) + 
    geom_line(color="blue") + geom_point(color="blue") +
    scale_x_continuous(breaks=bottleneckDims) +
    geom_line(aes(x=bottleneckDims, y=M_P)) + 
    geom_point(aes(x=bottleneckDims, y=M_P))

```

```{r visualize-metrics}
metricsDf <- cbind(
        bn =  bottleneckDims,
        as.data.frame(testList)
        #do.call(rbind.data.frame,testList)
)
```

# Session information

```{r}
sessionInfo()
```
