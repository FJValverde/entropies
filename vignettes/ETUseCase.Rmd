---
title: "Simple Use Case for the Entropy Triangle with ggtern"
author: "Francisco J. Valverde-Albacete"
date: "Nov, 28th,  2015"
output: html_document
---

# Environment construction
```{r}
# library(ggtern)
# library(entropy)  # To work out the appropriate coordinates.
library(caret)    # To build the classifiers.
library(mlbench)  # Many databases for ML tasks
library(entropies) # Processing and visualizing joint entropies
debugLevel <- 0 # Debug level 0-non-existent, 1-minimal
```

# Datasets available

The inventory of databases to be explored:

```{r}
# the inventory of databases you can access
#library(datasets)
name <- c("Ionosphere", "iris", "Glass") # 
classVar <- c(35, 5, 10)   # ordinal of the class attribute
className <- c("Class","Species", "Type")  # Name of class attribute
K <- c(2, 3, 7)  # No. of classes
datasets <- data.frame(name,classVar,className,K)

# To select a dataset by name
evalDataset <- function(dsName){
    dsName <- as.character(dsName)
    switch(dsName,
        "iris" = {data(iris); iris},
        "Ionosphere" = {data(Ionosphere); Ionosphere},
        "Glass"={data(Glass); Glass})
}
datasets
```

# Create the data

The data must be created from (multiclass) classifiers and then transformed into a data frame with the entropic measures.

We'll use the Fisher Iris data throughout, and a stratified partitioning of the database.

```{r}
dsRow <- datasets[2, ] # iris
dsRow <- datasets[3, ] #Glass
dsName <- dsRow$name
dataset <- evalDataset(dsName)
dsRow
summary(dataset)

# Basic data from the set for classification
X <- as.matrix(dataset[,-dsRow$classVar])
Y <- dataset[, dsRow$classVar]
# Eploring the label set
classes <- unique(Y)
numC <- length(classes)
print(sprintf("There are %d classes with distribution: ", numC))
summary(Y)
```

# A simple classifier evaluation

```{r}
#library(caret)
set.seed(2117)
#Basic data partitioning
inTrain <- createDataPartition(y=Y,
                               p=0.80, # Tries to do stratified sampling
                               list=FALSE)
trainX <- X[inTrain,]; trainY <- Y[inTrain]
testX <- X[-inTrain,]; testY <- Y[-inTrain]
#Basic model fitting
fit <- train(x=trainX, y=trainY, 
              method="knn",
              tuneLength = 15,
              preProcess = c("center", "scale"))
## obtain a training confusion matrix
trCM <- confusionMatrix(predict(fit,trainX), trainY) # caret::confusionMatrix
trTable <- trCM$table 
trEntropies <- entropies(trTable)
trCoords <- entropicCoords(trEntropies)

## prediction and the test confusion matrix
predicted <- predict(fit, testX)
teCM <- confusionMatrix(predicted,testY)
teTable <- teCM$table #table(predicted,testX[,5])
teCoords <- entropicCoords(entropies(teTable))
```

Printing the results in the Entropy Triangle for a single classification experiment:

```{r}
experiments <- rbind(cbind(trCoords, Phase="train", method="knn"),
                     cbind(teCoords, Phase="test", method="knn")
                     )
# The basic plot for the entropy triangle
# gp <- ggentropytern(experiments,)
# #plot training and testX in different colours and glyphs
# gp + geom_point(#data=experiments,
#                   aes(VIxy,MIxy2,DeltaHxy,colour=Phase, shape=Phase),
#                   size=3) +
#     scale_colour_brewer(palette="Set1")
gp <- ggentropytern(data=experiments, aes(colour=Phase), size=3)  +
    scale_colour_brewer(palette="Set1")
gp
```

Note that there is a suspicious behaviour in the plot in that the classifier achieves a better information transfer (correlated with accuracy) in testX than in training. 

This is part of the "evaluation paradox" for classifications: since the test must have a higher variance, there will be instances of train-test partitions where the performance on the test will be higher that on the train.

# A better picture with n-fold validation

To confirm this intuition and get all the value for our coin in the entropy trinagle, 
in the following, we use n-fold validation to visualize several experiments and their mean performance. 

First we create the folds: the number of folds is a parameter of this script.

```{r data partitioning}
numFolds <- 5
set.seed(1717) # For reproducibility
folds <- createFolds(Y, numFolds)
print("Check that the sampling was stratified...")
for(i in 1:numFolds){
    print(summary(Y[folds[[i]]]))
}
summary(Y)
```

Run the experiments

```{r n-fold validation}
models <- c("knn") #c("knn", "logreg") # c("logreg") 
results <- data.frame()
for (i in 1:numFolds){
    for (m in models){
        # 1. select training and testX data and classes
        trainObs <- unlist(folds[-i])
        testObs <- folds[[i]]
        trainX <- X[trainObs, ]; trainY <- Y[trainObs]
        testX <- X[testObs, ]; testY <- Y[testObs]
        # 2. Fit the model with the 
        model <- train(x=trainX, y=trainY, 
                       method=m,
                       tuneLength = 15,
                       preProcess = c("center", "scale"))
        # 3. Estimate the labels for the train set: confusion matrix, entropies, etc.
        trainYhat <- predict(model, trainX)
        trainCM <- confusionMatrix(trainYhat, trainY)
        print(trainCM$table)
        # 4. Estimate the labels for the test set
        testYhat <- predict(model, testX)
        testCM <- confusionMatrix(testYhat, testY)
        print(testCM$table)
        # 5. Gather results for analysis
        results <- rbind(results, 
                         cbind(evaluate(trainCM), Fold=i,method=m, Phase="train"),
                         cbind(evaluate(testCM), Fold=i, method=m, Phase="test")
        )
        print(sprintf("Fold %d, method %s Train accuracy = %f\t Test accuracy= %f", 
                      i, m, trainCM$overall[1],testCM$overall[1])
        )
    }
}
```

Now try to plot these results: disaggregate and aggregate...

```{r}
# eT <- ggentropytern(results) + 
#     geom_point(aes(VIxy,MIxy2,DeltaHxy, colour=Phase, shape=method)) +
#     #geom_confidence(color="blue",linetype=1) +
#     scale_colour_manual(values=c("black","red")) # Don't trust the training, that is the red
eT <- ggentropytern(data=results, aes(colour=Phase, shape=method), size=3)  +
    scale_colour_manual(values=c("black","red")) # Don't trust the training, that is the red
eT
#find mean and variance for train and test for: Accuracy, EMA, 
meanAccuracy <- aggregate(results$Accuracy, by= list(Phase=results$Phase), mean)
sdAccuracy <- aggregate(results$Accuracy, by= list(Phase=results$Phase), sd)
meanEMA <- aggregate(results$EMA, by= list(Phase=results$Phase), mean)
sdEMA <- aggregate(results$EMA, by= list(Phase=results$Phase), sd)
#try to plot on eT the mean results for testing and training:
byMethod <- results$method
byPhase <- results$Phase
meanResults <- results %>% 
    select(-one_of("Fold", "k", "m", "kx", "my", "muxy", "kx_y", "ky_x", 
                   "McnemarPValue", "method", "Phase")
           )
meanResults <- aggregate(meanResults, by=list(Phase=byPhase, method=byMethod), mean)
meanResults
eT + geom_point(data=meanResults, 
                aes(VIxy,MIxy2,DeltaHxy, colour=Phase, shape=method), 
                size=4)
```

# Visualization with the split triangle

To use the split triangle to advantage we have to use an unbalanced dataset, e.g. Glass from mlbench.

```{r}
library(mlbench)
dsRow <- datasets[3, ]
dsName <- dsRow$name
dataset <- evalDataset(dsName)
dsRow
summary(dataset)

# Basic data from the set for classification
X <- as.matrix(dataset[,-dsRow$classVar])
Y <- dataset[, dsRow$classVar]
# Eploring the label set
classes <- unique(Y)
numC <- length(classes)
print(sprintf("There are %d classes with distribution: ", numC))
summary(Y)
```


First we create the folds: the number of folds is a parameter of this script.

```{r data partitioning-2}
numFolds <- 5
set.seed(1717) # For reproducibility
folds <- createFolds(Y, numFolds)
print("Check that the sampling was stratified...")
for(i in 1:numFolds){
    print(summary(Y[folds[[i]]]))
}
summary(Y)
```

Run the experiments

```{r n-fold validation-2}
models <- c("knn") # c("knn", "logreg") 
results <- data.frame()
for (i in 1:numFolds){
    for (m in models){
        # 1. select training and testX data and classes
        trainObs <- unlist(folds[-i])
        testObs <- folds[[i]]
        trainX <- X[trainObs, ]; trainY <- Y[trainObs]
        testX <- X[testObs, ]; testY <- Y[testObs]
        # 2. Fit the model with the 
        model <- train(x=trainX, y=trainY, 
                       method=m,
                       tuneLength = 15,
                       preProcess = c("center", "scale"))
        # 3. Estimate the labels for the train set: confusion matrix, entropies, etc.
        trainYhat <- predict(model, trainX)
        trainCM <- confusionMatrix(trainYhat, trainY)
        print(trainCM$table)
        # 4. Estimate the labels for the test set
        testYhat <- predict(model, testX)
        testCM <- confusionMatrix(testYhat, testY)
        print(testCM$table)
        # 5. Gather results for analysis
        results <- rbind(results, 
                         cbind(evaluate(trainCM, split=TRUE), 
                               Fold=i, method=m, Phase="train"
                               ),
                         cbind(evaluate(testCM, split=TRUE), 
                               Fold=i, method=m, Phase="test"
                               )
                        )
        print(sprintf("Fold %d, method %s Train accuracy = %f\t Test accuracy= %f", 
                      i, m, trainCM$overall[1],testCM$overall[1])
        )
    }
}
```

Now try to plot these results in split coordinates: disaggregate and aggregate...

```{r}
# eT <- ggentropytern(results) + 
#     geom_point(aes(VIxy,MIxy2,DeltaHxy, colour=Phase, shape=method)) +
#     #geom_confidence(color="blue",linetype=1) +
#     scale_colour_manual(values=c("black","red")) # Don't trust the training, that is the red
eT <- ggentropytern(data=results, limit=FALSE) +
    geom_point(aes(colour=Phase)) +
    scale_colour_manual(values=c("black","red")) # Don't trust the training, that is the # eT <- ggentropytern(data=results, aes(colour=Phase, fill=method)) + 
#     scale_colour_manual(values=c("black","red")) # Don't trust the training, that is the red
eT
#find mean and variance for train and test for: Accuracy, EMA, 
meanAccuracy <- aggregate(results$Accuracy, by= list(Phase=results$Phase), mean)
sdAccuracy <- aggregate(results$Accuracy, by= list(Phase=results$Phase), sd)
meanEMA <- aggregate(results$EMA, by= list(Phase=results$Phase), mean)
sdEMA <- aggregate(results$EMA, by= list(Phase=results$Phase), sd)
#try to plot on eT the mean results for testing and training:
byMethod <- results$method
byPhase <- results$Phase
meanResults <- results %>% 
    select(-one_of("Fold", "k", "m", "kx", "my", "muxy", "kx_y", "ky_x", 
                   "McnemarPValue", "method", "Phase")
           )
meanResults <- aggregate(meanResults, by=list(Phase=byPhase, method=byMethod), mean)
meanResults
gatheredMeanResults <- gatherCoords(meanResults) # TRUE does not work!
eT <- eT + geom_point(data=gatheredMeanResults[which(gatheredMeanResults$Var %in% c("X", "Y")), ], 
                aes(colour=Phase, shape=Var), 
                size=4)
eT + geom_segment(data=gatheredMeanResults[which(gatheredMeanResults$Var == "Limit"), ],
                  aes(xend=VIEnd, yend=MIEnd, zend=DeltaEnd,
                      linetype=factor(linetype), colour=Phase),
                  size=0.5)
meT <- ggentropytern(data=meanResults, limit=TRUE) +
    geom_point(aes(colour=Phase)) + 
    scale_colour_manual(values=c("black","red")) # Don't trust the training, that is the
meT
```

# Session information

```{r}
sessionInfo()
```

