---
title: "Analysis of Confusion Matrices"
author: "F.J. Valverde-Albacete and C. Pelaez-Moreno"
date: "June, 21st, 2018"
output:
  pdf_document: default
  html_document: default
vignette: > 
  %\VignetteIndexEntry{A first guide to use the CBET for confusion matrix evaluation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
This script shows how to read a Channel Binary Entropy Triangle (CBET) using different confusion matrices. 

### Environment construction
```{r setup, include=FALSE}
library(tidyverse) #
library(entropies) # This package. Depends heavily on "ggtern", entropy", "infotheory".
```

Some top level switches and options gathered in one place. 
```{r switches}
debugLevel <- 0 # Debug level 0-non-existent, 1-minimal, the greater the more verbose.
fancy <- TRUE  # set this for nicer on-screen visualization.
#fancy <- FALSE # Set this for either printing matter or more austere plots.
getPlot <- TRUE # Flag to obtain plots for publication. 
getPlot <- FALSE #Comment to get .jpeg files rather than plots of ets.
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=4)
if (getPlot)
    knitr::opts_chunk$set(dev = 'pdf') # plots in pdf, better for publication
```

### Data

```{r, data}
mats <- list(
    a = c(15, 0, 0, 0, 15, 0, 5, 5, 20),
    b = c(16, 2, 1, 2, 16, 1, 2, 2, 18),
    c = c(1,0,1,0,1,1,4,4,48),
    d = c(15,0,0,0,18,0,0,0,27),
    e = c(1,0,0,0,2,0,0,0,57),
    #f = c(0,0,0,0,0,0,5,5,50)
    f = c(6,6,6,6,6,6,6,6,6)
)
asaTable <- function(m){matrix(m, nrow=3,ncol=3)}
cm <- lapply(mats,asaTable)
cmNames <- names(mats)
cm
```

### Entropies

```{r, entropies}
etdf <- data.frame()
for (i in 1:length(cmNames)) {
    etdf <- rbind(etdf,
                  cbind(name=cmNames[[i]], jentropies(as.table(cm[[i]])))
    )
}
```

# Visualizing entropies in the CBET

```{r, visualization}
confusion_cbet <- 
    ggmetern(etdf %>% filter(type == "XY"), fancy) +
    geom_point(aes(shape=name), size=3, color="blue") +
    labs(shape="Confusion Matrix") +
    theme(legend.key=element_blank())
confusion_cbet
if (getPlot){
    dev.off()#Necessary to do the textual plot.
    ggsave(stringr::str_interp("confusion_matrices_CBET_PRL10_in_R.jpeg"),
           plot=confusion_cbet)
}
```

The analysis of the classifiers generating the matrices is the following:

1. That one generating a is an good classifier, transferring a lot of information. That generating b is a little bit worse. d is generated by an excellent classifier, it can hardly get any better.

2. That one generating c is a worthless classifier: it makes almost random decisions about the class. Only f is more worthless. 

3. That one generating e has an accuracy of 1.0 to no avail, it really does not transmit a lot of information, partly because the class distribution is so unbalanced. 

# Visualizing Entropies in the split CBET

Sometimes we have to use the split diagram and it is customary to use the glyph shapes cross and empty circle to represent the X entropies (entering) and the Y entropies (exiting), but this is not enforced. 

The aggregate entropy (the one used in the previous diagram) can be plotted with a filled circle to see the nuances in these differences, although it is easy to see that this entropy always lies in the imaginary line between the split cross and circle. Check below!

In this case, we suggest using labels to annotate the working points for each classifier, as in the example below. 

```{r, split visualization}
confusion_split_cbet <- 
    ggmetern(etdf, fancy) %+% 
    geom_point(aes(shape=type), 
               size=3, 
               color="blue") +
    scale_shape_manual(values=c("X"=4, "Y"=1, "XY"=20)) +
    labs(shape="Split Confusion Matrix") +
    theme(legend.key=element_blank())
confusion_split_cbet <-  
    confusion_split_cbet + 
    geom_text(data=etdf %>% filter(type == "XY"), aes(label=name), 
              color="blue",size=4, vjust=2, hjust=1)
confusion_split_cbet
```

Some points are worth noticing:

* If the cardinality of the input and output classes is the same, the three points---X entropy, Y entropy and aggregate---lie on an imaginary line from the X entropy to the Y entropy that cannot exceed the triangle boundaries. 

* Since the entropy of X is fixed, the only way for the aggregate entropy to move is for the Y entropy to move. Y entropies "tend" to move to the right, with present-day technologies, that is, trying to gain accuracy without increasing the overall mutual information. This is commonly referred to as "cheating".

The following is a just a charm to plot the diagram for publishing purposes.

```{r last plot}
if (getPlot){
    dev.off()#Necessary to do the textual plot.
    ggsave(stringr::str_interp("confusion_matrices_split_CBET_PRL10_in_R.jpeg"),
           plot=confusion_split_cbet)
}
```
# Postscriptum

More information about the evaluation of classifiers with the Channel Binary Entropy Triangle can be found in 

```{r echo=FALSE}
library(bibtex)
print(citation("entropies")['val:pel:14a'], style="text")
```

# Session information

```{r}
sessionInfo()
```
